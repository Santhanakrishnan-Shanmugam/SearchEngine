{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c07d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\krish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: requests in c:\\users\\krish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\krish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\krish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\krish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\krish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\krish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\krish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0461969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url='https://en.wikipedia.org/wiki/Artificial_intelligence'\n",
    "\n",
    "response=requests.get(url)\n",
    "content=response.text\n",
    "\n",
    "soup=BeautifulSoup(content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89bffc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Artificial intelligence - Wikipedia\n",
      "\n",
      " Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1]\n",
      "\n",
      "URL: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "\n",
      "2. Generative artificial intelligence - Wikipedia\n",
      "\n",
      " Generative artificial intelligence (Generative AI, GenAI,[1] or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data.[2][3][4] These models learn the underlying patterns and structures of their training data and use them to produce new data[5][6] based on the input, which often comes in the form of natural language prompts.[7][8]\n",
      "\n",
      "URL: https://en.wikipedia.org/wiki/Generative_artificial_intelligence\n",
      "\n",
      "3. AI boom - Wikipedia\n",
      "\n",
      " The AI boom[1][2] is an ongoing period of technological progress in the field of artificial intelligence (AI) that started in the late 2010s before gaining international prominence in the 2020s. Examples include generative AI technologies, such as large language models and AI image generators by companies like OpenAI, as well as scientific advances, such as protein folding prediction led by Google DeepMind. This period is sometimes referred to as an AI spring, to contrast it with previous AI winters.[3][4] As of 2025, ChatGPT is the 5th most visited website globally behind Google, YouTube, Facebook, and Instagram.[5][6]\n",
      "\n",
      "URL: https://en.wikipedia.org/wiki/AI_boom\n",
      "\n",
      "4. Artificial intelligence visual art - Wikipedia\n",
      "\n",
      " Artificial intelligence visual art, or AI art, is visual artwork generated (or enhanced) through the use of artificial intelligence (AI) programs.\n",
      "Automated art has been created since ancient times. The field of artificial intelligence was founded in the 1950s, and artists began to create art with artificial intelligence shortly after the discipline was founded. Throughout its history, AI has raised many philosophical concerns related to the human mind, artificial beings, and also what can be considered art in human–AI collaboration. Since the 20th century, people have used AI to create art, some of which has been exhibited in museums and won awards.[1]\n",
      "\n",
      "URL: https://en.wikipedia.org/wiki/Artificial_intelligence_visual_art\n",
      "\n",
      "5. Perplexity AI - Wikipedia\n",
      "\n",
      " Perplexity AI, Inc., or simply Perplexity, is an American privately held software company offering a web search engine that processes user queries and synthesizes responses.[4] Perplexity products use large language models and incorporate real-time web search capabilities, providing responses based on current Internet content, including inline source citations.[5] A free public version is available, while a paid Pro subscription offers access to more advanced language models and additional features.[6]\n",
      "\n",
      "URL: https://en.wikipedia.org/wiki/Perplexity_AI\n",
      "\n",
      "6. AI alignment - Wikipedia\n",
      "\n",
      " In the field of artificial intelligence (AI), alignment aims to steer AI systems toward a person's or group's intended goals, preferences, or ethical principles. An AI system is considered aligned if it advances the intended objectives. A misaligned AI system pursues unintended objectives.[1]\n",
      "\n",
      "URL: https://en.wikipedia.org/wiki/AI_alignment\n",
      "\n",
      "7. Ethics of artificial intelligence - Wikipedia\n",
      "\n",
      " The ethics of artificial intelligence covers a broad range of topics within AI that are considered to have particular ethical stakes.[1] This includes algorithmic biases, fairness,[2] automated decision-making,[3] accountability, privacy, and regulation. It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation,[4] how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.[1]\n",
      "\n",
      "URL: https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence\n",
      "\n",
      "8. AI literacy - Wikipedia\n",
      "AI literacy or artificial intelligence literacy is the ability to understand, use, monitor, and critically reflect on AI applications.[1] The term usually refers to teaching skills and knowledge to the general public, particularly those who are not adept in AI.[1]\n",
      " Some think AI literacy is essential for school and college students,[1][2] while some professors ban AI in the classroom and from all assignments[3] with stern punishments for using AI, classifying it as cheating.[4]  AI is employed in a variety of applications, including self-driving automobiles, virtual assistants and text generation by generative AI models. Users of these tools should be able to make informed decisions. AI literacy may have an impact students' future employment prospects.[1]\n",
      "\n",
      "URL: https://en.wikipedia.org/wiki/AI_literacy\n",
      "\n",
      "9. OpenAI - Wikipedia\n",
      "\n",
      " OpenAI, Inc. is an American artificial intelligence (AI) organization headquartered in San Francisco, California. It aims to develop \"safe and beneficial\" artificial general intelligence (AGI), which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\".[6] As a leading organization in the ongoing AI boom,[7] OpenAI is known for the GPT family of large language models, the DALL-E series of text-to-image models, and a text-to-video model named Sora.[8][9] Its release of ChatGPT in November 2022 has been credited with catalyzing widespread interest in generative AI.\n",
      "\n",
      "URL: https://en.wikipedia.org/wiki/OpenAI\n",
      "\n",
      "10. Machine learning - Wikipedia\n",
      "\n",
      " Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions.[1] Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.[2]\n",
      "\n",
      "URL: https://en.wikipedia.org/wiki/Machine_learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote, urljoin\n",
    "import time\n",
    "\n",
    "# Crawl a single page and extract title + snippet\n",
    "def crawl_page(url):\n",
    "    headers = {\"User-Agent\": \"MiniCrawler/1.0 (+https://example.com)\"}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return None\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Extract title\n",
    "        title = soup.title.string if soup.title else \"No title\"\n",
    "        \n",
    "        # Extract first 2 paragraphs as snippet\n",
    "        paragraphs = soup.find_all('p')\n",
    "        snippet = \" \".join([p.get_text() for p in paragraphs[:2]])\n",
    "        \n",
    "        return {\"url\": url, \"title\": title, \"snippet\": snippet}\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to crawl {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 1: Get dynamic URLs from Wikipedia search\n",
    "def get_wiki_urls(query, max_urls=10):\n",
    "    encoded_query = quote(query)\n",
    "    search_url = f\"https://en.wikipedia.org/w/index.php?search={encoded_query}\"\n",
    "    headers = {\"User-Agent\": \"MiniCrawler/1.0 (+https://example.com)\"}\n",
    "    urls = []\n",
    "    try:\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Get links from search results\n",
    "        for a_tag in soup.select(\"div.mw-search-result-heading > a\"):\n",
    "            link = urljoin(\"https://en.wikipedia.org\", a_tag['href'])\n",
    "            if link not in urls:\n",
    "                urls.append(link)\n",
    "            if len(urls) >= max_urls:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get search URLs: {e}\")\n",
    "    return urls\n",
    "\n",
    "# Step 2: Crawl pages dynamically\n",
    "def dynamic_crawl(query, max_pages=10):\n",
    "    urls = get_wiki_urls(query, max_urls=max_pages)\n",
    "    results = []\n",
    "    for url in urls:\n",
    "        page = crawl_page(url)\n",
    "        if page:\n",
    "            results.append(page)\n",
    "        time.sleep(1)  # polite delay\n",
    "    return results\n",
    "\n",
    "# Step 3: Run crawler\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter your query: \")\n",
    "    results = dynamic_crawl(query, max_pages=10)\n",
    "    \n",
    "    for i, page in enumerate(results, 1):\n",
    "        print(f\"{i}. {page['title']}\\n{page['snippet']}\\nURL: {page['url']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18b67f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a56bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Algorithm in Machine Learning - GeeksforGeeks\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"https://www.geeksforgeeks.org/machine-learning/gradient-descent-algorithm-and-its-variants/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extract title\n",
    "if soup.title:\n",
    "    print(soup.title.string)\n",
    "else:\n",
    "    print(\"No Title\")\n",
    "for a_tag in soup.select(\"div.mw-search-result-heading > a\"):\n",
    "    print(a_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d41f2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Artificial intelligence - Wikipedia',\n",
       "  'content': 'Artificial intelligence(AI) is the capability ofcomputational systemsto perform tasks typically associated withhuman intelligence, such aslearning,reasoning,problem-solving,perception, anddecision-making. It is afield of researchincomputer sciencethat develops and studies methods andsoftwarethat enable machines toperceive their environmentand uselearningandintelligenceto take actions that maximize their chances of achieving defined goals.[1]',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Artificial_intelligence'},\n",
       " {'title': 'Generative artificial intelligence - Wikipedia',\n",
       "  'content': 'Generative artificial intelligence(Generative AI,GenAI,[1]orGAI) is a subfield ofartificial intelligencethat usesgenerative modelsto produce text, images, videos, or other forms of data.[2][3][4]These modelslearnthe underlying patterns and structures of theirtraining dataand use them to produce new data[5][6]based on the input, which often comes in the form of natural languageprompts.[7][8]',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'},\n",
       " {'title': 'AI boom - Wikipedia',\n",
       "  'content': 'TheAI boom[1][2]is an ongoing period oftechnological progress inthe field ofartificial intelligence(AI) that started in the late 2010s before gaining international prominence in the 2020s. Examples includegenerative AItechnologies, such aslarge language modelsandAI image generatorsby companies likeOpenAI, as well as scientific advances, such asprotein folding predictionled byGoogle DeepMind. This period is sometimes referred to as anAI spring, to contrast it with previousAI winters.[3][4]As of 2025,ChatGPTis the5th most visited websiteglobally behindGoogle,YouTube,Facebook, andInstagram.[5][6]',\n",
       "  'url': 'https://en.wikipedia.org/wiki/AI_boom'},\n",
       " {'title': 'Perplexity AI - Wikipedia',\n",
       "  'content': 'Perplexity AI, Inc., or simplyPerplexity, is an Americanprivately heldsoftware companyoffering a websearch enginethatprocessesuser queries and synthesizes responses.[4]Perplexity products uselarge language modelsand incorporate real-time web search capabilities, providing responses based on currentInternetcontent, including inline source citations.[5]A free public version is available, while a paid Pro subscription offers access to more advanced language models and additional features.[6]',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Perplexity_AI'},\n",
       " {'title': 'Artificial intelligence visual art - Wikipedia',\n",
       "  'content': 'Artificial intelligence visual art, orAI art, isvisual artworkgenerated (or enhanced) through the use ofartificial intelligence(AI) programs.\\nAutomated art has been created since ancient times. The field of artificial intelligence was founded in the 1950s, and artists began to create art with artificial intelligence shortly after the discipline was founded. Throughoutits history, AI has raised manyphilosophical concernsrelated to thehuman mind,artificial beings, and also what can be consideredartin human–AI collaboration. Since the 20th century, people have used AI to create art, some of which has been exhibited in museums and won awards.[1]',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Artificial_intelligence_visual_art'},\n",
       " {'title': 'AI alignment - Wikipedia',\n",
       "  'content': \"In the field ofartificial intelligence(AI),alignmentaims to steer AI systems toward a person's or group's intended goals, preferences, or ethical principles. An AI system is consideredalignedif it advances the intended objectives. AmisalignedAI system pursues unintended objectives.[1]\",\n",
       "  'url': 'https://en.wikipedia.org/wiki/AI_alignment'},\n",
       " {'title': 'OpenAI - Wikipedia',\n",
       "  'content': 'OpenAI, Inc.is an Americanartificial intelligence(AI) organization headquartered inSan Francisco, California. It aims to develop \"safe and beneficial\"artificial general intelligence(AGI), which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\".[6]As a leading organization in the ongoingAI boom,[7]OpenAI is known for theGPTfamily oflarge language models, theDALL-Eseries oftext-to-image models, and atext-to-video modelnamedSora.[8][9]Its release ofChatGPTin November 2022 has been credited with catalyzing widespread interest ingenerative AI.',\n",
       "  'url': 'https://en.wikipedia.org/wiki/OpenAI'},\n",
       " {'title': 'Ethics of artificial intelligence - Wikipedia',\n",
       "  'content': 'Theethicsofartificial intelligencecovers a broad range of topics within AI that are considered to have particular ethical stakes.[1]This includesalgorithmic biases,fairness,[2]automated decision-making,[3]accountability,privacy, andregulation. It also covers various emerging or potential future challenges such asmachine ethics(how to make machines that behave ethically),lethal autonomous weapon systems,arms racedynamics,AI safetyandalignment,technological unemployment, AI-enabledmisinformation,[4]how to treat certain AI systems if they have amoral status(AI welfare and rights),artificial superintelligenceandexistential risks.[1]',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence'},\n",
       " {'title': 'AI literacy - Wikipedia',\n",
       "  'content': \"AI literacyorartificial intelligence literacyis the ability to understand, use, monitor, and critically reflect onAIapplications.[1]The term usually refers to teaching skills and knowledge to the general public, particularly those who are not adept in AI.[1]Some think AI literacy is essential for school and college students,[1][2]while some professors ban AI in the classroom and from all assignments[3]with stern punishments for using AI, classifying it as cheating.[4]AI is employed in a variety of applications, includingself-driving automobiles,virtual assistantsand text generation bygenerative AI models. Users of these tools should be able to make informed decisions. AI literacy may have an impact students' future employment prospects.[1]\",\n",
       "  'url': 'https://en.wikipedia.org/wiki/AI_literacy'},\n",
       " {'title': 'Machine learning - Wikipedia',\n",
       "  'content': 'Machine learning(ML) is afield of studyinartificial intelligenceconcerned with the development and study ofstatistical algorithmsthat can learn fromdataandgeneraliseto unseen data, and thus performtaskswithout explicitinstructions.[1]Within a subdiscipline in machine learning, advances in the field ofdeep learninghave allowedneural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.[2]',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Machine_learning'},\n",
       " {'title': 'Meta AI - Wikipedia',\n",
       "  'content': 'Meta AIis a research division ofMeta(formerlyFacebook) that developsartificial intelligenceandaugmented realitytechnologies.Meta AIwas founded in 2013 and was titled Facebook Artificial Intelligence Research (FAIR) withYann LeCunas its director.Vladimir Vapnikjoined the group in 2014.[1][2]It has workspaces inMenlo Park, California,London, United Kingdom, andManhattan.[3]',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Meta_AI'}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def page(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"MiniCrawler/1.0 (+https://example.com)\"}\n",
    "        response=requests.get(url,headers=headers)\n",
    "        if response.status_code!=200:\n",
    "            print(response.status_code)\n",
    "            return None\n",
    "        soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "        title=soup.title.string if soup.title else 'No title'\n",
    "        para=soup.find_all('p')\n",
    "        content=''.join([p.get_text(strip=True) for p in para[:2]])\n",
    "        return {'title':title,\n",
    "                'content':content,\n",
    "                'url':url\n",
    "                }\n",
    "    except  Exception as e:\n",
    "        return e\n",
    "def dyna_c(query,max_result=10):\n",
    "    encoded_query=quote(query)\n",
    "    search_url = f\"https://en.wikipedia.org/w/index.php?search={encoded_query}\"\n",
    "    headers = {\"User-Agent\": \"MiniCrawler/1.0 (+https://example.com)\"}\n",
    "    text=requests.get(search_url,headers=headers)\n",
    "    soup=BeautifulSoup(text.text,\"html.parser\")\n",
    "    urls=[]\n",
    "    for atag in soup.select(\"div.mw-search-result-heading > a\"):\n",
    "        url=urljoin(\"https://en.wikipedia.org\",atag['href'])\n",
    "        urls.append(url)\n",
    "\n",
    "        if len(urls)>max_result:\n",
    "            break\n",
    "    results=[]\n",
    "    for url in urls[:]:\n",
    "        result=page(url)\n",
    "        results.append(result)\n",
    "    return results    \n",
    "\n",
    "query=\"what is AI\"    \n",
    "dyna_c(query,max_result=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17e35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NotificationsIntroduction to Machine LearningTypes of Machine LearningWhat is Machine Learning Pipeline?Applications of Machine Learning\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = \"https://www.geeksforgeeks.org/machine-learning/gradient-descent-algorithm-and-its-variants/\"\n",
    "headers = {\"User-Agent\": \"MiniCrawler/1.0 (+https://example.com)\"}\n",
    "response=requests.get(url,headers)\n",
    "soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "para=soup.find_all('p')\n",
    "content=''.join([p.get_text(strip=True) for p in para[:]])\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5cbc1c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notifications\n",
      "Introduction to Machine Learning\n",
      "Types of Machine Learning\n",
      "What is Machine Learning Pipeline?\n",
      "Applications of Machine Learning\n",
      "Machine Learning with Python Tutorial\n",
      "NumPy Tutorial - Python Library\n",
      "Pandas Tutorial\n",
      "Data Preprocessing in Python\n",
      "EDA - Exploratory Data Analysis in Python\n",
      "What is Feature Engineering?\n",
      "Introduction to Dimensionality Reduction\n",
      "Feature Selection Techniques in Machine Learning\n",
      "Supervised Machine Learning\n",
      "Linear Regression in Machine learning\n",
      "Logistic Regression in Machine Learning\n",
      "Decision Tree in Machine Learning\n",
      "Random Forest Algorithm in Machine Learning\n",
      "K-Nearest Neighbor(KNN) Algorithm\n",
      "Support Vector Machine (SVM) Algorithm\n",
      "Naive Bayes Classifiers\n",
      "What is Unsupervised Learning\n",
      "K means Clustering – Introduction\n",
      "Hierarchical Clustering in Machine Learning\n",
      "DBSCAN Clustering in ML - Density based clustering\n",
      "Apriori Algorithm\n",
      "Frequent Pattern Growth Algorithm\n",
      "ECLAT Algorithm - ML\n",
      "Principal Component Analysis(PCA)\n",
      "Evaluation Metrics in Machine Learning\n",
      "Regularization in Machine Learning\n",
      "Cross Validation in Machine Learning\n",
      "Hyperparameter Tuning\n",
      "ML | Underfitting and Overfitting\n",
      "Bias and Variance in Machine Learning\n",
      "Reinforcement Learning\n",
      "Semi-Supervised Learning in ML\n",
      "Self-Supervised Learning (SSL)\n",
      "Ensemble Learning\n",
      "Top 50+ Machine Learning Interview Questions and Answers\n",
      "100+ Machine Learning Projects with Source Code [2025]\n",
      "Gradient descentis the backbone of the learning process for various algorithms, including linear regression, logistic regression, support vector machines, and neural networks which serves as a fundamental optimization technique to minimize the cost function of a model byiteratively adjusting the model parameters to reduce the difference between predicted and actual values, improving the model's performance. Let's see it's role in machine learning:\n",
      "Prerequisites: Understand the working and math of gradient descent.\n",
      "Neural networksare trained using Gradient Descent (or its variants) in combination withbackpropagation. Backpropagation computes the gradients of theloss function with respect to each parameter (weights and biases) in the network by applying thechain rule.The process involves:\n",
      "Gradients are then used by Gradient Descent to update the parameters layer-by-layer, moving toward minimizing the loss function.\n",
      "Neural networks often use advanced variants of Gradient Descent. If you want to read more about variants, please refer :Gradient Descent Variants.\n",
      "The algorithm minimizes a cost function, which quantifies the error or loss of the model's predictions compared to the true labels for:\n",
      "Gradient descent minimizes theMean Squared Error (MSE)which serves as the loss function to find the best-fit line. Gradient Descent is used to iteratively update the weights (coefficients) and bias by computing the gradient of the MSE with respect to these parameters.\n",
      "Since MSE is a convex functiongradient descent guarantees convergence to the global minimum if the learning rate is appropriately chosen.For each iteration:\n",
      "The algorithm computes the gradient of the MSE with respect to the weights and biases.\n",
      "It updates the weights (w) and bias (b) using the formula:\n",
      "w = w - \\alpha \\cdot \\frac{\\partial J(w, b)}{\\partial w}, \\quad b = b - \\alpha \\cdot \\frac{\\partial J(w, b)}{\\partial b}\n",
      "The formula is theparameter update rule for gradient descent, which adjusts the weights w and biases b to minimize a cost function. This process iteratively adjusts the line's slope and intercept to minimize the error.\n",
      "In logistic regression, gradient descent minimizes theLog Loss (Cross-Entropy Loss)to optimize the decision boundary for binary classification. Since the output is probabilistic (between 0 and 1), the sigmoid function is applied. The process involves:\n",
      "w = w - \\alpha \\cdot \\frac{\\partial J(w)}{\\partial w}\n",
      "This adjustment shifts the decision boundary to separate classes more effectively.\n",
      "For SVMs, gradient descent optimizes thehinge loss, which ensures a maximum-margin hyperplane. The algorithm:\n",
      "Gradient descent ensures theoptimal placement of the hyperplane to separate classes with the largest possible margin.\n",
      "Diving further into the concept, let's understand in depth, with practical implementation.\n",
      "Output:\n",
      "The number of weight values will be equal to the input size of the model, And the input size in deep Learning is the number of independent input features i.e we are putting inside the model\n",
      "In our case, input features are two so, the input size will also be two, and the corresponding weight value will also be two.\n",
      "Output:\n",
      "Output:\n",
      "Here we are calculating the Mean Squared Error by taking the square of the difference between the actual and the predicted value and then dividing it by its length (i.e n = the Total number of output or target values) which is the mean of squared errors.\n",
      "Output:\n",
      "As we can see from the above right now the Mean Squared Error is 30559.4473. All the steps which are done till now are known as forward propagation.\n",
      "Now our task is to find the optimal value of weight w and bias b which can fit our model well by giving very less or minimum error as possible. i.e\n",
      "Now to update the weight and bias value and find the optimal value of weight and bias we will do backpropagation. Here the Gradient Descent comes into the role to find the optimal value weight and bias.\n",
      "For the sake of complexity, we can write our loss function for the single row as below\n",
      "In the above function x and y are our input data i.e constant. To find the optimal value of weight w and bias b. we partially differentiate with respect to w and b. This is also said that we will find the gradient of loss function J(w,b) with respect to w and b to find the optimal value of w and b.\n",
      "\\begin {aligned} {J}'_w &=\\frac{\\partial J(w,b)}{\\partial w} \\\\ &= \\frac{\\partial}{\\partial w} \\left[\\frac{1}{n} (y_p-y)^2 \\right] \\\\ &= \\frac{2(y_p-y)}{n}\\frac{\\partial}{\\partial w}\\left [(y_p-y)  \\right ] \\\\ &= \\frac{2(y_p-y)}{n}\\frac{\\partial}{\\partial w}\\left [((xW^T+b)-y)  \\right ] \\\\ &= \\frac{2(y_p-y)}{n}\\left[\\frac{\\partial(xW^T+b)}{\\partial w}-\\frac{\\partial(y)}{\\partial w}\\right] \\\\ &= \\frac{2(y_p-y)}{n}\\left [ x - 0 \\right ] \\\\ &= \\frac{1}{n}(y_p-y)[2x] \\end {aligned}\n",
      "i.e\n",
      "\\begin {aligned} {J}'_w &= \\frac{\\partial J(w,b)}{\\partial w} \\\\ &= J(w,b)[2x] \\end{aligned}\n",
      "\\begin {aligned} {J}'_b &=\\frac{\\partial J(w,b)}{\\partial b} \\\\ &= \\frac{\\partial}{\\partial b} \\left[\\frac{1}{n} (y_p-y)^2 \\right] \\\\ &= \\frac{2(y_p-y)}{n}\\frac{\\partial}{\\partial b}\\left [(y_p-y)  \\right ] \\\\ &= \\frac{2(y_p-y)}{n}\\frac{\\partial}{\\partial b}\\left [((xW^T+b)-y)  \\right ] \\\\ &= \\frac{2(y_p-y)}{n}\\left[\\frac{\\partial(xW^T+b)}{\\partial b}-\\frac{\\partial(y)}{\\partial b}\\right] \\\\ &= \\frac{2(y_p-y)}{n}\\left [ 1 - 0 \\right ] \\\\ &= \\frac{1}{n}(y_p-y)[2] \\end {aligned}\n",
      "i.e\n",
      "\\begin {aligned} {J}'_b &= \\frac{\\partial J(w,b)}{\\partial b} \\\\ &= J(w,b)[2] \\end{aligned}\n",
      "Here we have considered the linear regression. So that here the parameters are weight and bias only. But in a fully connected neural network model there can be multiple layers and multiple parameters.  but the concept will be the same everywhere. And the below-mentioned formula will work everywhere.\n",
      "Here,\n",
      "In our case:\n",
      "In the current problem, two input features, So, the weight will be two.\n",
      "Steps:\n",
      "Output:\n",
      "From the above graph and data, we can observe the Losses are decreasing as per the weight and bias variations.\n",
      "Now we have found the optimal weight and bias values. Print the optimal weight and bias and\n",
      "Output:\n",
      "Output:\n",
      "Thelearning rateis a critical hyperparameter in the context of gradient descent, influencing the size of steps taken during the optimization process to update the model parameters. Choosing an appropriate learning rate is crucial for efficient and effective model training.\n",
      "When the learning rate istoo small, the optimization process progresses very slowly. The model makes tiny updates to its parameters in each iteration, leading to sluggish convergence and potentially getting stuck in local minima.\n",
      "On the other hand, anexcessively large learning ratecan cause the optimization algorithm to overshoot the optimal parameter values, leading to divergence or oscillations that hinder convergence.\n",
      "Achieving the right balance is essential. A small learning rate might result in vanishing gradients and slow convergence, while a large learning rate may lead to overshooting and instability.\n",
      "Vanishing and exploding gradientsare common problems that can occur during the training of deep neural networks. These problems can significantly slow down the training process or even prevent the network from learning altogether.\n",
      "The vanishing gradient problem occurs when gradients become too small during backpropagation. The weights of the network are not considerably changed as a result, and the network is unable to discover the underlying patterns in the data. Many-layered deep neural networks are especially prone to this issue. The gradient values fall exponentially as they move backward through the layers, making it challenging to efficiently update the weights in the earlier layers.\n",
      "The exploding gradient problem, on the other hand, occurs when gradients become too large during backpropagation. When this happens, the weights are updated by a large amount, which can cause the network to diverge or oscillate, making it difficult to converge to a good solution.\n",
      "There are several variants of gradient descent that differ in the way the step size or learning rate is chosen and the way the updates are made. Here are some popular variants:\n",
      "Inbatch gradient descent, To update the model parameter values like weight and bias, the entire training dataset is used to compute the gradient and update the parameters at each iteration. This can be slow for large datasets but may lead to a more accurate model. It is effective for convex or relatively smooth error manifolds because it moves directly toward an optimal solution by taking a large step in the direction of the negative gradient of the cost function. However, it can be slow for large datasets because it computes the gradient and updates the parameters using the entire training dataset at each iteration. This can result in longer training times and higher computational costs.\n",
      "InSGD, only one training example is used to compute the gradient and update the parameters at each iteration. This can be faster than batch gradient descent but may lead to more noise in the updates.\n",
      "InMini-batch gradient descenta small batch of training examples is used to compute the gradient and update the parameters at each iteration. This can be a good compromise between batch gradient descent and Stochastic Gradient Descent, as it can be faster than batch gradient descent and less noisy than Stochastic Gradient Descent.\n",
      "Inmomentum-based gradient descent, Momentum is a variant of gradient descent that incorporates information from the previous weight updates to help the algorithm converge more quickly to the optimal solution. Momentum adds a term to the weight update that is proportional to the running average of the past gradients, allowing the algorithm to move more quickly in the direction of the optimal solution. The updates to the parameters are based on the current gradient and the previous updates. This can help prevent the optimization process from getting stuck in local minima and reach the global minimum faster.\n",
      "Nesterov Accelerated Gradient (NAG) is an extension of Momentum Gradient Descent. It evaluates the gradient at a hypothetical position ahead of the current position based on the current momentum vector, instead of evaluating the gradient at the current position. This can result in faster convergence and better performance.\n",
      "InAdagrad, the learning rate is adaptively adjusted for each parameter based on the historical gradient information. This allows for larger updates for infrequent parameters and smaller updates for frequent parameters.\n",
      "InRMSpropthe learning rate is adaptively adjusted for each parameter based on the moving average of the squared gradient. This helps the algorithm to converge faster in the presence of noisy gradients.\n",
      "Adamstands for adaptive moment estimation, it combines the benefits of Momentum-based Gradient Descent, Adagrad, and RMSprop the learning rate is adaptively adjusted for each parameter based on the moving average of the gradient and the squared gradient, which allows for faster convergence and better performance on non-convex optimization problems. It keeps track of two exponentially decaying averages the first-moment estimate, which is the exponentially decaying average of past gradients, and the second-moment estimate, which is the exponentially decaying average of past squared gradients. The first-moment estimate is used to calculate the momentum, and the second-moment estimate is used to scale the learning rate for each parameter. This is one of the most popular optimization algorithms for deep learning.\n",
      "In the intricate landscape of machine learning and deep learning, the journey of model optimization revolves around the foundational concept of gradient descent and its diverse variants. Through the lens of this powerful optimization algorithm, we explored the intricacies of minimizing the cost function, a pivotal task in training models.\n",
      "C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in para:\n",
    "    text=p.getText(strip=True)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7071f9a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Chroma.__init__() got an unexpected keyword argument 'embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     embedding\u001b[38;5;241m.\u001b[39mappend(out\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     24\u001b[0m docs \u001b[38;5;241m=\u001b[39m [Document(page_content\u001b[38;5;241m=\u001b[39mt) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text]\n\u001b[1;32m---> 26\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_collection\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:817\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[0;32m    798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[0;32m    799\u001b[0m \n\u001b[0;32m    800\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 817\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[1;32mc:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:226\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    225\u001b[0m     emit_warning()\n\u001b[1;32m--> 226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Chroma.__init__() got an unexpected keyword argument 'embeddings'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import torch\n",
    "from langchain_core.documents import Document\n",
    "text=[\n",
    "\"Notifications\",\n",
    "'Introduction to Machine Learning',\n",
    "'Types of Machine Learning',\n",
    "'What is Machine Learning Pipeline?',\n",
    "'Applications of Machine Learning',\n",
    "'Machine Learning with Python Tutorial',\n",
    "'NumPy Tutorial - Python Library',\n",
    "'Pandas Tutorial']\n",
    "\n",
    "model_name=\"sentence-transformers/all-MiniLM-L6-V2\"\n",
    "model=AutoModel.from_pretrained(model_name)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "embedding=[]\n",
    "for t in text:\n",
    "    tokens = tokenizer(t, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        out=model(**tokens)\n",
    "    embedding.append(out.last_hidden_state.mean(dim=1).squeeze(0).numpy())\n",
    "docs = [Document(page_content=t) for t in text]\n",
    "\n",
    "db = Chroma.from_texts(\n",
    "    texts=[doc.page_content for doc in docs],\n",
    "    embedding=None,            \n",
    "    metadatas=[doc.metadata for doc in docs],\n",
    "    ids=None,\n",
    "    persist_directory=None,\n",
    "    collection_name=\"my_collection\",\n",
    "    embeddings=embedding  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a107d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class TransformerEmbeddings:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for t in texts:\n",
    "            tokens = self.tokenizer(t, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                out = self.model(**tokens)\n",
    "            emb = out.last_hidden_state.mean(dim=1).squeeze(0).numpy()\n",
    "            embeddings.append(emb)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e354097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction to Machine Learning\n",
      "Notifications\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "texts = [\"Notifications\", \"Introduction to Machine Learning\"]\n",
    "docs = [Document(page_content=t) for t in texts]\n",
    "\n",
    "embedding_model = TransformerEmbeddings(\"sentence-transformers/all-MiniLM-L6-V2\")\n",
    "db = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# ✅ Now you can query\n",
    "query = \"Machine Learning tutorial\"\n",
    "results = db.similarity_search(query, k=3)\n",
    "for r in results:\n",
    "    print(r.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db7aa716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Dict\n",
    "from urllib.parse import quote, urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "class Agentstate(TypedDict):\n",
    "    query: 'what is ai'\n",
    "    results: List[Dict[str, str]]\n",
    "    db: object\n",
    "    top_k_results: List[Dict[str, str]]\n",
    "    llm_answer: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c5eea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling(state: Agentstate):\n",
    "    query = state['query']\n",
    "    encoded_query = quote(query)\n",
    "    search_url = f\"https://en.wikipedia.org/w/index.php?search={encoded_query}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    results = []\n",
    "    atags = soup.select(\"div.mw-search-result-heading > a\")\n",
    "\n",
    "    if atags:  # search results exist\n",
    "        for atag in atags[:10]:\n",
    "            url = urljoin(\"https://en.wikipedia.org/\", atag['href'])\n",
    "            results.append(page(url))\n",
    "    else:  # redirected to single article\n",
    "        results.append(page(response.url))\n",
    "\n",
    "    state['results'] = results\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f08ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "crawling() missing 1 required positional argument: 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ans\u001b[38;5;241m=\u001b[39m\u001b[43mcrawling\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: crawling() missing 1 required positional argument: 'state'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
